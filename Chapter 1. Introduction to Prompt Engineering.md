# 第一章 提示词工程简介

ChatGPT 于 2022 年 11 月下旬发布。到次年 1 月，这款应用的月活跃用户估计已达到 1 亿，成为史上增长最快的消费级应用。相比之下，TikTok 用了 9 个月才达到 1 亿用户，而 Instagram 则用了 2 年半。尊敬的读者，相信您也会认可，这份公众赞誉实至名归！

支撑 ChatGPT 的大语言模型（LLM）正在彻底改变我们的工作方式。过去如果想寻找答案，我们会先打开 Google 进行传统的网页搜索；现在，直接向大语言模型提问即可轻松获得话题讲解。过去解决技术难题时，我们可能会翻阅 Stack Overflow 或在博客文章中苦苦搜寻；如今，可以让大语言模型为你的特定问题空间量身定制教程，并继续深入开展问答（Q&A）。过去要构建一个编程库，通常得按部就班地完成所有步骤；现在，只需与大语言模型助理配合，它就能帮你搭好脚手架，在编码时自动补全，从而大大提升你的进度！

亲爱的未来读者，你是否会以我们这些 2024 年的作者难以想象的方式来使用大语言模型？如果目前的趋势继续下去，你很可能在一天中会多次与大语言模型进行对话：在你家网线断了时，它会是 IT 支持人员的声音；在街角的 ATM 机旁，它会与你进行友好交谈；它甚至会是一个令人沮丧的、极其逼真的自动推销电话。

与大语言模型的互动远不止此。大语言模型会为你筛选新闻，总结你最可能感兴趣的头条，并移除（或者可能添加）带有偏见的评论。你将使用大语言模型来辅助你的沟通，比如撰写和总结电子邮件，办公室和家庭助理甚至会代表你进入现实世界进行交流和操作。在一天之内，你的个人 AI 助手可能在某一刻扮演旅行社的代理，帮你规划行程、预订机票和酒店；而在另一刻，它又扮演购物助手，帮你找到并购买你需要的物品。

为什么大语言模型如此神奇？因为它们就是魔法！正如未来学家亚瑟·克拉克（Arthur C. Clarke）的名言：“任何足够先进的技术都与魔法无异。” 我们认为，一台可以与之交谈的机器当然称得上是魔法，但这本书的目标正是要揭开这层魔法的面纱。我们将证明，无论大语言模型有时看起来多么不可思议、多么直观、多么像人，其核心都只是一个预测文本块中下一个词的模型，仅此而已！

因此，大语言模型仅仅是帮助用户完成某些任务的工具，而你与这些工具互动的方式就是编写提示词（prompt），也就是它们要补全的那段文本的开端。这就是我们所说的**提示词工程（prompt engineering）**。通过这本书，我们将为提示词工程建立一个实用的框架，并最终构建有大语言模型功能的应用，这将为你应用的用户带来一种神奇的体验。

本章为你即将开始的提示词工程之旅奠定了背景。但首先，让我们告诉你，我们这些作者是如何亲身发现这其中的魔力的。


## 大语言模型就是魔法

本书的两位作者都是 GitHub Copilot 代码补全产品的早期研究开发者。Albert 是创始团队成员，而当 Albert 转向其他更前沿的大语言模型研究项目时，John 加入了进来。

Albert 早在 2020 年年中就首次发现了这种魔力。他是这样描述的：

> 在我们机器学习代码小组的创意会议上，大约每半年就会有人提起代码合成（code synthesis）这个话题。而答案总是一样的：它将会非常惊艳，但至少还得再等五年。这就像我们的“冷核聚变”——人人都期待，却始终遥不可及。
>
> 这种情况一直持续，直到我第一次上手体验那个后来被称为 OpenAI Codex 的早期大语言模型原型。那时我才看到，未来已来：冷核聚变终于实现了。
>
> 很明显，这个模型与我们以前所知的那些糟糕的代码合成尝试完全不同。它不仅是**有可能**预测下一个词——它还能仅凭函数注释（docstring）就生成完整的语句，甚至整个函数。而且这些函数真的能跑起来！
>
> 在我们决定用这个模型打造什么之前（剧透：它最终成为了 GitHub 的 Copilot 代码补全产品），我们想量化一下这个模型到底有多好。于是，我们找到了一群 GitHub 工程师，让他们提出一些独立的编码任务。有些任务相对简单——但这些人都是硬核程序员，他们的许多任务也相当复杂。很多任务是那种初级开发者会求助于谷歌的，而有些甚至能让高级开发者去查 Stack Overflow。然而，如果我们给模型几次尝试的机会，它就能解决其中大部分问题。
>
> 那一刻我们确信——这正是能够引领全新编码时代的“引擎”。而我们要做的，只是在它周围打造一辆恰如其分的“交通工具”。

对 John 来说，那个神奇的时刻发生在几年后的 2023 年初，当时他正在测试这个“交通工具”并驾驶它兜风。他是这样回忆的：

> 我启动了屏幕录制，并介绍了我准备解决的编码挑战：创建一个函数，输入一个整数，返回该数字对应的英文写法。例如，如果输入是 `10`，输出就是 "ten"；如果输入是 `1,004,712`，输出就是 "one million four thousand seven hundred twelve"。
>
> 这比你想象的要难，因为拜英语所赐，奇怪的例外比比皆是。`10` 到 `20` 之间的数字的文本版本不遵循任何其他十位数的模式("eleven"、"twelve" 和那些以 "teen" 结尾的词不同)；十位数的数字也打破了预期的模式，例如，如果 `90` 是 "ninety"，`80` 是 "eighty"，那为什么 `30` 不是 "threety"，`20` 不是 "twoty" 呢？但我编码挑战中真正的难点在于，我想用一种我**从未接触过**的语言来实现这个解决方案——Rust。Copilot 能胜任这个挑战吗？
>
> 通常，在学习一门新的编程语言时，我会参考典型的入门教程：如何创建变量？如何创建 list？如何遍历 list 中的 item？如何编写 if 语句？但有了 Copilot，我直接开始写函数注释（docstring）：
>
> ```rust
> // GOAL: Create a function that prints a string version of any number （目标：创建一个函数，打印任意数字的字符串形式）
> // supplied to the function. （该数字由函数接收）
> // 1 -> "one"
> // 2034 -> "two thousand thirty four"
> // 11 -> "eleven"
> fn
> ```
>
> Copilot 看到了 `fn`，就跳出来帮忙：
>
> ```rust
> fn number_to_string(number: i32) -> String {
> ```
>
> 完美！我当时并不知道如何为函数的输入参数和返回值标注类型，但随着我们继续协作，我通过注释来引导高级别的工作流程，比如“将输入数字按三位一组拆分”，而 Copilot 则有效地教我各种编程结构。这包括如何创建一个向量并将其赋给变量，如 `let mut number_string_vec = Vec::new();`，以及如何编写循环，如 `while number > 0 {`。
>
> 这次体验非常棒。我既在取得进展，又在学习这门语言，重要的是我没有因为不断查阅语言教程而分心——我的项目本身就是我的教程。然后，在这个实验进行了 20 分钟后，Copilot 让我大吃一惊。我输入了一行注释，并开始了下一个我知道我们需要的控制循环：
>
> ```rust
> // iterate through number_string_vec, assemble the name of the number
> // for each order of magnitude, and concatenate to number_string
> //
> // 遍历 number_string_vec，按每个数量级（个位、十位、百位等）组装该数字的名称，
> // 并将其拼接到 number_string 上
> for
> ```
>
> 稍作停顿后，Copilot 插入了 30 行代码！[在录屏中，你甚至可以听到我倒吸一口凉气的声音](https://github.blog/developer-skills/github/4-ways-github-engineers-use-github-copilot/#4-exploring-and-learning)。代码成功编译，语法完全正确，并且运行了。
>
>答案有点小问题，输入 `5,034,012` 得到的结果是 "five thirty four thousand twelve million"，但是，我也不指望一个人第一次就能做对，而且这个 bug 很容易发现和修正。
>
>在 40 分钟的结对编程结束后，我完成了一件不可能的事——**用一门我完全不熟悉的语言编写了非同小可的代码**！Copilot 指导我基本理解了 Rust 的语法，并且它对我目标的抽象理解更深，在几个关键点上介入帮助我填补细节。如果我自己尝试，我估计会花上好几个小时。

我们这些神奇的经历并非个例。如果你正在读这本书，你很可能自己也与大语言模型有过一些令人震撼的互动。也许你最初是通过 ChatGPT 认识到大语言模型的力量，又或者，你的首次体验来自 2023 年初以来层出不穷的第一代应用：微软 Bing、Google Bard 等搜索助手，或是微软 Copilot 系列等文档助手。但是，抵达今天这个技术拐点并非一蹴而就。要真正理解大语言模型，我们必须先了解这条发展之路是如何走来的。

-----

## 语言模型：我们是如何走到今天的？

要理解我们是如何走到技术史上这个非常有趣的节点，我们首先需要知道语言模型到底是什么，它做什么。还有谁比世界上最受欢迎的大语言模型应用 ChatGPT 更适合回答这个问题呢？（见[图 1-1](#figure1_1)）

```
> 你:
> 什么是语言模型？

> ChatGPT:
> 语言模型是一种人工智能系统，它通过训练来理解和生成类似人类的文本。它通过处理大量的文本数据来学习语言的结构、语法和语义。语言模型的主要目标是在给定上下文中预测下一个词或词序列的概率。

```
![](./illustrations/chapter%2001/Figure1-1.png)

<span id="figure1_1">图 1-1</span>. 什么是语言模型？

看吧？就像我们在本章开头说的那样：语言模型的主要目标是预测下一个词的概率。你以前见过这个功能，不是吗？就是你在 iPhone 上发短信时，键盘上方出现的那一格候选词栏（见[图 1-2](#figure1_2)）。你可能从来没注意过它……因为它并不那么好用。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/9a311ee61e2a0b9beefba23f23bae986c609aba49b20e67a098ee53d7c75c7d7.jpg) 

<span id="figure1_2">图 1-2</span>. John 指着他手机上的候选词栏

如果这就是语言模型的全部功能，那它们究竟是如何席卷全球的呢？

### 早期的语言模型

语言模型实际上已经存在很长时间了。如果你是在本书出版后不久知道它，那么驱动 iPhone “猜下一个词”功能的语言模型是基于 1948 年首次提出的自然语言[马尔可夫模型](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)。然而，还有其他一些更近期的语言模型，它们更直接地为正在进行的人工智能革命奠定了基础。

到 2014 年，最强大的语言模型是基于[谷歌推出的序列到序列（seq2seq）架构](https://arxiv.org/abs/1409.3215)。Seq2seq 是一种循环神经网络（recurrent neural network），理论上它应该非常适合文本处理，因为它一次处理一个 Token（词元），并循环更新其内部状态。

这使得 seq2seq 能够处理任意长度的文本序列。通过专门的架构和训练，seq2seq 架构能够执行几种不同类型的自然语言任务：分类、实体提取、翻译、摘要等等。但这些模型有一个致命弱点——信息瓶颈限制了它们的能力。

seq2seq 架构有两个主要部分：编码器（encoder）和解码器（decoder）（见[图 1-3](#figure1_3)）。处理过程首先向编码器发送一串 Tokens，编码器逐个处理这些 Token。在接收 Token 时，编码器会更新一个隐藏状态向量（hidden state vector），这个向量会累积来自输入序列的信息。当最后一个 Token 处理完毕后，这个隐藏状态的最终值，被称为“思维向量”（thought vector），会被发送到解码器。然后，解码器利用来自思维向量的信息来生成输出 Token。但问题是，思维向量是固定且有限的（固定维度的向量）。它常常会“忘记”较长文本块中的重要信息，导致解码器可用的信息不足——这就是**信息瓶颈**。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/cfc336545643dbe3b8750d71aacad702105fcd6e294cdbff292b3eeacce88462.jpg)  
<span id="figure1_3">图 1-3</span>. 一个翻译用的 seq2seq 模型

图中的模型工作流程如下：

1.  源语言的 Token 被逐一发送到编码器，转换为嵌入向量（embedding vector），并更新编码器的内部状态。
2.  内部状态被打包成思维向量并发送给解码器。
3.  一个特殊的“开始” Token 被发送到解码器，表示这是输出 Token 的开始。
4.  基于思维向量的值，解码器状态被更新，并输出一个目标语言的 Token。
5.  输出的 Token 作为下一个输入提供给解码器。此时，过程在第 4 步和第 5 步之间循环往复。
6.  最后，解码器输出一个特殊的“结束” Token，表示解码过程完成。有限的思维向量只能向解码器传递有限的信息。

一篇 2015 年的论文[《通过联合学习对齐和翻译实现神经机器翻译》（Neural Machine Translation by Jointly Learning to Align and Translate）](https://arxiv.org/abs/1409.0473)引入了一种新方法来解决这个瓶颈。编码器不再只提供一个单一的思维向量，而是保留了在编码过程中为遇到的每个词元生成的所有隐藏状态向量，然后允许解码器对所有这些向量进行“软搜索”,也就是对它们加权求和，以便在需要时关注最相关的信息。作为演示，该论文表明，在一个英法翻译模型中使用软搜索显著提高了翻译质量。这种软搜索技术很快被称为**注意力机制（attention mechanism）**。

注意力机制很快在人工智能社区引起了广泛的关注，最终在 2017 年谷歌研究院的论文[《注意力就是你所需要的一切》（Attention Is All You Need）](https://arxiv.org/abs/1706.03762)中达到顶峰，该论文介绍了如[图 1-4]((#figure1_4)) 所示的 **Transformer** 架构。Transformer 保留了其前身的高层结构——先由编码器接收 Token 输入，再由解码器生成输出 Token。但与 seq2seq 模型不同，所有的循环神经结构都被移除了，Transformer 完全依赖于注意力机制。由此产生的架构非常灵活，并且显著提升了对训练数据的建模能力。但是，与能够处理任意长度序列的 seq2seq 相比，Transformer 只能处理固定且**有限长度**的输入和输出序列。由于 Transformer 是 GPT 模型的直系祖先，这一长度限制也就成为我们此后不断努力突破的瓶颈。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/29d178cea001e47f4020f7c1ae09ea959adfce971e18b6d03cddac07a054fa32.jpg)
<span id="figure1_4">图 1-4</span>. Transformer 架构


## GPT 登场

2018 年的论文[《通过生成式预训练提升语言理解》（Improving Language Understanding by Generative Pre-Training）](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) 首次提出了生成式预训练 Transformer（Generative Pre-trained Transformer）架构。这个架构在当时并不特别或新颖。实际上，这个架构只是把标准 Transformer 的编码器拆掉，只保留了解码器部分。然而，正是这种简化，在随后的几年里带来了意想不到的可能性。也正是这种生成式预训练 Transformer 架构**GPT**，点燃了正在进行的人工智能革命。

在 2018 年，这一点并不明显。当时的主流做法是，先用无标签数据（例如，来自互联网的文本片段）对模型进行预训练，然后修改模型架构并进行专门的微调（fine-tuning），以便最终模型只擅长**一项**任务。

GPT 架构也是如此：先在无标签文本上进行预训练，然后针对特定任务进行监督式微调，可以在各种任务（如分类、衡量文档间相似性、回答多选题）上表现优异。但我们应该强调一点：经过微调后，GPT 只擅长它被微调的那一项任务。

**GPT-2** 只是 GPT 的一个放大版。当它在 2019 年推出时，研究人员开始意识到 GPT 架构的与众不同之处。这一点在介绍 GPT-2 的 [OpenAI 博客文章的第二段](https://openai.com/index/better-language-models/)中就能明显看出：

> 我们的模型名为 GPT-2（GPT 的继任者），仅仅通过预测 40GB 互联网文本中的下一个词进行训练。出于对该技术可能被恶意应用的担忧，我们不会发布这个训练好的模型。

哇！这两句话怎么能并列在一起？像在 iPhone 上发短信时那种仅仅预测下一个词的无害功能，怎么会导致对滥用的如此严重担忧？如果你读那篇对应的学术论文[《语言模型是无监督的多任务学习者》（Language Models Are Unsupervised Multitask Learners）](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)，你就会开始找到答案。

GPT-2 有 15 亿个参数，而 GPT 只有 1.17 亿；它在 40GB 的文本上训练，而 GPT 是 4.5GB。模型和训练集规模的简单数量级增长，带来了一种前所未有的**涌现特性**：无需为单一任务微调，只用预训练好的 GPT-2 直接上手，就往往能超过那些专门为该任务微调的最先进模型。

这种现象出现在多项基准测试上，包括了理解模糊代词、预测文本中缺失的词、词性标注等等。即便在阅读理解、摘要、翻译和问答任务中，GPT-2 略逊于当时的最优模型，它的表现仍然令人惊讶，而那些对手可都是为各自任务精细微调过的模型。

但是，为什么会对这个模型的“恶意应用”有那么多担忧呢？这是因为它已经变得非常擅长模仿自然语言文本。正如 OpenAI 博客文章所指出的，这种能力可以被用来“生成误导性的新闻文章、在网上冒充他人、自动化生产用于社交媒体的辱骂性或伪造内容，以及自动化生产垃圾邮件/钓鱼内容。” 如果说 2019 年这一潜在风险已经令人不安，那么到了今天，这种可能性只会更加真实，问题也更加严峻。

**GPT-3** 在模型大小和训练数据上又实现了一个数量级的增长，其能力也相应地实现了飞跃。2020 年的论文[《语言模型是少样本学习者》（Language Models Are Few-Shot Learners）](https://arxiv.org/abs/2005.14165)表明，只要给出几个你希望模型完成的任务示例（即“少样本示例”），模型就能精准复现输入模式，从而执行几乎所有你能想象到的基于语言的任务，并且常常得到的结果质量惊人。就在这时，我们发现可以通过调整输入——即**提示词（prompt）**——就能让模型执行所需任务。这就是提示词工程（prompt engineering）的诞生。

**ChatGPT** 于 2022 年 11 月发布，背后依托的正是 GPT-3.5——接下来的故事就是历史了！但这是一段正在迅速书写的历史（见[表 1-1](#table1_1)）。2023 年 3 月，**GPT-4** 发布，虽然细节没有公开透露，但据传该模型在模型大小和训练数据量上又增加了一个数量级，并且其能力再次远超前一代。

从那时起，越来越多的模型相继出现。有些来自 OpenAI，另一些则来自主要的行业参与者，如 Meta 的 Llama、Anthropic 的 Claude 和谷歌的 Gemini。我们不断见证质量的飞跃，而且越来越多体积更小、速度更快的模型也能达到同样的水准。可以说，这一进步的脚步只会愈发加快。

<span id="table1_1">表 1-1<span>. GPT 系列模型的详细信息，显示了所有指标的指数级增长

| 模型 | 发布日期 | 参数数量 | 训练数据 | 训练成本 |
| :--- | :--- | :--- | :--- | :--- |
| GPT-1 | 2018年6月11日 | 1.17 亿 | BookCorpus：来自 7000 本各种类型未出版书籍的 4.5 GB 文本 | 1.7e19 FLOPs |
| GPT-2 | 2019年2月14日(初版)；2019年11月5日(完整版) | 15 亿 | WebText：来自 4500 万个在 Reddit 上被点赞的网页的 40 GB 文本和 800 万份文档 | 1.5e21 FLOPs |
| GPT-3 | 2020年5月28日 | 1750 亿 | 4990 亿个词元，包括 Common Crawl (570 GB)、WebText、英文维基百科和两个图书语料库 (Books1 和 Books2) | 3.1e23 FLOPs |
| GPT-3.5 | 2022年3月15日 | 1750 亿 | 未公开 | 未公开 |
| GPT-4 | 2023年3月14日 | 1.8 万亿 (传闻) | 传闻为 13 万亿个词元 | 估计为 2.1e25 FLOPs |

-----

## 提示词工程

现在，我们来到了你进入提示词工程世界旅程的起点。归根结底，大语言模型只擅长一件事——补全文本。输入到模型的那段文字被称为**提示词**，它是一份我们期望模型去补全的文档或文本块。因此，最简单的提示词工程，就是精心设计提示词，让模型生成的补全文本恰好包含解决问题所需的信息。

在本书中，我们呈现的提示词工程视角远不止“写好一个提示词”这么简单，我们讨论的是整个基于 LLM 的应用流程，其中提示词的构建和答案的解释都是以编程方式完成的。

为了构建一个高质量的软件和优质的用户体验（UX），提示词工程师必须在用户、应用程序和 LLM 之间的迭代通信之间创建一个通信模式。用户向应用程序描述他们的问题，应用程序构建一个要发送给 LLM 的“伪文档”，LLM 补全该文档，最后，应用程序解析补全内容并将结果传达回用户，或代表用户执行操作。

提示词工程的科学与艺术在于，如何让这三方的沟通架构，既能准确映射用户的“问题域”，又能适配 LLM 的“文档域”。

提示词工程有几个不同的复杂层次。最基本的形式只使用一个非常薄的应用层。例如，当你与 ChatGPT 互动时，其实就是在直接编写提示词；应用端只不过用特定的 ChatML Markdown 把整条对话包起来。（你将在[第三章](#第三章)学到更多相关内容。）同样，GitHub Copilot 诞生之初，也不过是把当前文件内容直接交给模型补全。

下一个复杂层次，提示词工程涉及到修改或增强用户输入。例如，LLM 处理的是文本，所以一个技术支持热线可以将用户的语音转录成文本，并用在发送给 LLM 的提示词中。此外，还可以把之前工单记录或相关支持文档的内容也包含在提示词中。

举个现实世界的例子，随着 GitHub Copilot 代码补全功能的发展，我们意识到，如果我们将用户邻近标签页中的相关代码片段也整合进来，补全质量会显著提高。这很合理，对吧？用户打开那些标签页是因为他们正在参考那里的信息，所以模型也能从这些信息中受益是理所当然的。

另一个例子是新版 Bing 聊天式搜索，在这种情况下，来自传统搜索结果的内容被注入提示词中。这使得助手能够讨论它在训练数据中从未见过的信息（例如，因为它涉及到模型训练后发生的事件）。更重要的是，这种方法帮助减少**幻觉（hallucinations）**，从下一章开始,这个话题我们将在全书中多次探讨。

在这个复杂层次上，提示词工程更高一级的层次是当与 LLM 的互动需要保存**状态（stateful）** 时，即它们会保持先前互动的上下文和信息。聊天应用是这里的典型例子。对于用户的每一次新交流，应用程序都必须回忆起先前交流中发生的事情，并生成一份完整提示词。随着对话或历史记录变长，你必须小心避免提示词过大，或包含可能分散模型注意力的无关内容。你可能会选择丢弃最早的交流或先前交流中不太相关的内容，甚至可能使用摘要来压缩上下文。

在这个复杂层次上，提示词工程更进一步，是基于 LLM 的应用程序**工具**，让 LLM 能够通过发出 API 请求来接触现实世界，以读取信息，甚至创建或修改互联网上可用的资产。

例如，一个基于 LLM 的电子邮件应用可能会收到用户这样的输入：“给黛安发一封 5 月 5 日的会议邀请。” 这个应用程序会使用一个工具在用户的联系人列表中识别出黛安，然后使用一个日历 API 查找她的空闲时间，最后再发送一封电子邮件邀请。

随着这些模型变得越来越便宜和强大，想象一下用我们今天已有的 API 能实现多少可能性！这里的提示词工程至关重要。模型将如何知道使用哪个工具？它将如何以正确的方式使用工具？你的应用程序将如何正确地与模型共享工具执行的信息？当工具使用导致某种错误状态时我们该怎么办？我们将在[第八章](#第八章)讨论所有这些问题。

我们在本书中涵盖的最后一个复杂层次是如何为 LLM 应用程序提供**自主性（agency）**——即它能够自己决定如何完成用户提供的宽泛目标。这是 LLM 能力的前沿领域，研究与实践都在进行。你已经可以下载 [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 并给它一个目标，它就会启动一个多步骤的过程来收集完成目标所需的信息。它总能成功吗？不。实际上，除非目标非常受限，否则它失败的次数往往比成功的次数多。但是，赋予 LLM 应用程序某种形式的自主性，仍然是通往未来激动人心可能性的关键一步。在[第八章](#第八章)和[第九章](#第九章)将会分享我们的观点。

## 结论

正如我们开头所说，本章为你即将开始的提示词工程之旅奠定了背景。我们首先回顾了语言模型的最新发展历程，并强调了大语言模型为何如此特殊与不同——以及为什么它们正在推动我们所见证的这场人工智能革命。然后，我们定义了本书的主题：提示词工程。

特别要说明的是，你应该理解本书并不仅仅教你怎么字斟句酌地写出一条提示词，来获得一个好的补全。当然，我们会涵盖这一点，并且我们会详细说明生成高质量补全所需的各种技巧。但是当我们说“提示词工程”时，我们指的是构建**完整的基于大语言模型的应用程序**。这个 LLM 应用程序作为一个转换层，能够迭代且有状态地把现实需求转化为大语言模型可处理的文本，再把模型生成的数据转回可真正解决现实需求的信息或行动。

在我们开始这段旅程之前，让我们确保我们已经做好了充分的准备。在下一章，你将学习 LLM 文本补全是**如何工作**的，从顶层 API 一直到底层的注意力机制。在随后的章节中，我们将在此基础上解释 LLM 是如何被扩展以处理**聊天**和**工具使用**的，你会看到，归根到底它们其实都是一回事——文本补全。然后，掌握了这些基础知识，就为你接下来的旅程做好了准备。