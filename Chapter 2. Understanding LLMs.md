
# 第二章：理解大语言模型

你想不想成为一位“大语言模型调校师”，用巧妙的提示词来解锁大语言模型丰富的知识和强大的处理能力？

要真正领会什么样的提示词才算巧妙，并能从大语言模型那里“引诱”出正确的答案，你首先需要理解大语言模型是如何处理信息的，也就是它们是如何“思考”的。

在本章中，我们将像剥洋葱一样层层深入地探讨这个问题。首先，在[“什么是大语言模型？”](#什么是大语言模型)一节，你会从最表层看到，大语言模型是经过训练的文本模仿者。

接着，在[“大语言模型如何看待世界”](#大语言模型如何看待世界)中，你将了解到它们如何将文本拆分成一个个称为“Token”的小块，并且会了解到，如果拆分不顺利，将会带来什么不良后果。

随后，在[“一次一个 Token”](#一次一个-token)中，你会发现 Token 序列是如何被逐个生成的，并在[“温度与概率”](#温度与概率)中学到选择下一个 Token 的不同方式。

最后，在[“Transformer 架构”](#transformer-架构)中，你将深入大语言模型的最核心工作原理，看到它其实是由许多“小脑”组成，这些小脑通过名为“注意力”（attention）的问答游戏相互沟通，并了解这对提示词的顺序意味着什么。

在阅读过程中，请记住，这是一本关于**如何使用**大语言模型的书，而不是一本关于大语言模型**本身**的书。因此，我们省略了许多很酷的技术细节，因为它们与提示词工程无关。

如果你想了解矩阵乘法和激活函数，你需要查阅其他资料，比如经典的[《图解 Transformer》](https://jalammar.github.io/illustrated-transformer/)是深度探索的绝佳起点。但如果你只想写出色的提示词，了解本章涉猎的内容就足够了。现在，就让我们开始吧。

## 什么是大语言模型？

从最基础的角度来看，一个大语言模型就是一个**输入文本，输出文本**的服务：你给它一串文本、它再返回一串文本。输入的文本被称为**提示词 (prompt)**，而输出的文本则被称为**补全 (completion)**，有时也叫**回复 (response)**（见[图 2-1](#figure2_1)）。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/2ff5c6db1a12fd1deef6df303163e71c6626e4a359ad6603d4bc5a662ea937f6.jpg)  
<span id="figure2_1">图 2-1</span>. 一个大语言模型接收到提示词“One, Two,”（一，二），并给出补全“Buckle My Shoe”（系鞋带）

>译者注：
>
>“One, two, buckle my shoe” （一、二，系鞋带）是一首英语童谣（nursery rhyme）的开头两句，常用来教小朋友数数。
>
>类似于中文的“一二三四五，上山打老虎”。


当一个未经训练的大语言模型刚“出生”时，它补全的内容看起来就像一堆随机的 Unicode 符号，与提示词之间没有任何明确的联系。它需要经过**训练**才能变得有用。训练之后，它才会“开窍”：这时候将不再是简单地用字符串回应字符串，而是用语言回应语言。

然而，训练需要大量的技巧、算力和时间，这远远超出了大多数项目团队的能力范围。因此，大多数大语言模型应用都使用现成的通用模型（被称为**基础模型 foundation models**），这些模型已经训练完毕（可能经过一些[微调；见旁注](#什么是微调fine-tuning)）。

所以，我们并不期望你自己去训练一个大语言模型，但如果你想使用大语言模型，尤其是在编程中使用，那么理解它是**被训练来做什么的**至关重要。

> #### **什么是微调（FINE-TUNING）？**
>
> 训练大语言模型需要海量的数据和计算资源，不过，许多基础知识，比如英语语法规则，在不同的训练数据集中并无太大差异。因此，在训练一个大语言模型时，通常不会完全从零开始，而是会从另一个大语言模型的副本进行，这个副本可能是在不同的文档上训练得来的。
>
> 例如，OpenAI Codex（一个为 GitHub Copilot 开发的、用于生成源代码的大语言模型）的早期版本，就是一个现有模型（GPT-3，一个自然语言大语言模型）的副本，然后用 GitHub 上发布的大量源代码进行了**微调 (fine-tuning)**而成。
>
> 如果你有一个模型，在数据集 A 上训练、然后在数据集 B 上做了微调，你的提示词通常应该写得就像它完全是在数据集 B 上的一样。我们将在[第七章](#第七章)更深入地探讨微调。

大语言模型（LLM）通过使用称为**训练集（training set）**的大量文档（同样是字符串）进行训练。训练集所含文档的类型取决于大语言模型的用途（见[图 2-2](#figure2_2) 的例子）。

训练集往往混合了多种输入，例如书籍、文章、Reddit 等平台上的对话，以及 GitHub 等网站上的代码。模型需要从训练集中学习，生成看起来与训练集非常相似的输出。

具体来说，当模型收到的提示词是其训练集中某篇文档的开头时，它生成的补全，应该是最有可能延续原始文档的文本。换句话说，**模型是在模仿**。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/47b0d4fa96131342184e49c0cd557f2efc198f5121e4b2e32e5be2b0d59b6f0e.jpg)  
<span id="figure2_2">图 2-2</span>. [开源训练集 “The Pile”](https://github.com/EleutherAI/the-pile) 的组成，包含了事实性散文、虚构散文、对话以及其他互联网内容混合组成。



[“The Pile” 数据集](https://github.com/EleutherAI/the-pile)有效来源构成

* **Books (书籍):** 12.1%
* **Gutenberg (PG-19) (古腾堡计划):** 2.2%
* **PubMed Central (生物医学文献库):** 14.4%
* **PubMed Abstracts (PubMed 摘要):** 3.1%
* **ArXiv (科研论文预印本):** 9.0%
* **FreeLaw (法律文件):** 6.1%
* **USPTO Backgrounds (美国专利商标局背景):** 3.7%
* **OpenSubtitles (电影字幕):** 1.6%
* **Ubuntu IRC (Ubuntu 实时聊天记录):** 0.9%
* **Pile-CC (基于 Common Crawl 的数据):** 18.1%
* **OpenWebText (网页文本):** 10.0%
* **Stack Exchange (问答网站):** 5.1%
* **Wikipedia (en) (维基百科-英文):** 1.5%
* **DM Mathematics (数学论文):** 1.2%
* **Github (代码库):** 7.6%

---

>译者注：
>
>Gutenberg (PG-19)：“古腾堡计划”（Project Gutenberg）一个著名的数字图书馆，志愿者们致力于将版权已过期的书籍数字化并免费提供给公众。PG-19是一个特定的数据集的名称，指的是它包含的书籍都是在 1919 年之前出版的。
>
>Ubuntu IRC 是一个包含了大量关于 Ubuntu 系统的技术讨论和问答的聊天记录数据集。
>
>Pile-CC (基于 Common Crawl 的数据):Common Crawl (简称 CC) 是一个非营利组织，它会持续地抓取（crawl）整个互联网。Pile-CC 是 "The Pile" 数据集的创建者们从 Common Crawl (CC) 中提取并处理后得到的数据集。

那么，大语言模型和把全部训练数据做成的大型搜索引擎索引有什么区别呢？毕竟，搜索引擎在完成大语言模型训练的任务上会表现得非常出色，比如给定一篇文档的开头，它能以 100% 的准确率找到这篇文档的后续内容。

然而，我们这里的目标并不是要一个只会鹦鹉学舌复述训练集的搜索引擎：大语言模型不应该学会死记硬背训练集，而是应该学会**应用**在训练集中遇到的模式（特别是逻辑和推理模式）来补全**任何**提示词，而不仅仅是运用训练集里包含的那些。单纯的死记硬背（rote memorization）被认为是一种缺陷。为了防止这种缺陷，大语言模型的内部架构鼓励它从具体例子中抽象出规律，训练过程也尝试提供多样化、不重复的数据，并在未见过的数据上评估效果。

这种预防措施有时会失败，模型没有学会事实和模式，反而死记硬背下大段文本，这被称为**过拟合 (overfitting)**。大规模的过拟合在现成的模型中应该很少见，但值得注意的是，如果一个大语言模型看似解决了一个它在训练中见过的问题，这并不一定意味着当它面对一个一类相似却未见过的问题时，也能同样出色。

在你使用大语言模型一段时间后，你会逐渐形成一种“直觉”，能根据它所训练任务预测它的行为。 所以，当你想知道一个给定的提示词可能会如何被补全时，与其设想“一个理性的人”会如何“回答”，不如想象，**如果一篇恰好以这个提示词开头的文档，它接下来最有可能怎么写下去？**

>  **💡 提示**
>
> 假设你从训练集中随机抽取了一篇文档。你所知道的只是：它的开头正好就是你的提示词。那么，从统计角度看，接下来最有可能出现的文字是什么？这就是你应该期待的大语言模型给出的输出。

> 译者注： 
>「训练集中随机抽取了一篇文档，开头正好就是你的提示词的前提下，大语言模型输出的是最符合训练语料统计规律的继续内容」。如果训练集中只有一篇文档以该提示开头，那就是它原本的后续；若有多篇，则是这些后续中出现频率（或概率质量）最高的那一段。

### 补全一篇文档

这里有一个关于文档补全推理的例子。思考以下文本：

> Yesterday, my TV stopped working. Now, I can't turn it on at
>
>(昨天，我的电视机坏了。现在，我完全无法把它打_)

对于这样开头的文本，统计上最可能的补全是什么？

1. y2ior3w  
2. Thursday.
3. all.

这些补全没有一个是绝对不可能的。有时，猫会从键盘上跑过，生成了补全 1；其他时候，句子在重写时被弄乱，出现了补全 2。但到目前为止，最可能的续写是 3，几乎所有的大语言模型都会选择这个续写。

让我们假设补全 3 是确定的，然后让大语言模型再往前走一步：

> Yesterday, my TV stopped working. Now, I can't turn it on at all.
>
>(昨天，我的电视机坏了。现在，我完全无法把它打开。)

对于这样开头的文本，统计上最可能的补全是什么？

-a.  这就是为什么我今晚选择安下心来看书。
-b.  我们去你家看比赛怎么样？
-c.  首先，试试把电视的插头从墙上拔下来，再重新插回去。

嗯，这取决于训练集。假设这个大语言模型是在一个叙事性散文的数据集上训练的，比如短篇小说、长篇小说、杂志和报纸。在这种情况下，关于读书的补全 `a` 听起来比其他的更有可能。

虽然关于电视的句子，后面跟着补全 `b` 的问题，很可能出现在故事的中间部分，但一个故事不会用这个问题来开头，至少得有个起始引号（“）作为对话的开始。所以，一个在短篇故事上训练的模型不太可能预测选项 `b`。

但是，如果在训练集中加入电子邮件和对话记录，突然之间，选项 `b` 就显得非常合理了。不过，这两个都是我编的：第三个选项才是一个实际的大语言模型（OpenAI 的 text-davinci-003，GPT-3 的一个变体）生成的，它模仿了其训练集中大量的建议和客服对话。

这里逐渐浮现出一个核心观点：**你越了解训练数据，你就能对基于该训练数据训练出的大语言模型的可能输出形成越好的直觉**。许多商业大语言模型不公开它们的训练数据，选择一个好的训练集是它们模型成功的“独家秘方”。但即便如此，我们通常还是可以对训练集包含哪类文档形成一些合理的预期。

### 人类思维 vs 大语言模型处理

大语言模型会选择看起来最可能的续写，这一点与人类阅读文本时的某些预期并不一致。这是因为当人类写作时，所做的不仅仅是生成看起来合理的文字，还伴随着一系列的额外动作。

比如说，你想写一篇关于你在播客网站 Acast 上偶然发现的一个播客的博客文章。你可能会开始这样写：
> “在他们最新一期的‘The rest is history’中，他们谈论了百年战争（在 acast 上收听，网址： http://）。” 

你当然记不住那个网址，所以这时候你会停下写作，快速上网搜索一下。运气好的话，你会找到正确的链接：[shows.acast.com/the-rest-is-history-podcast/episodes/321-hundred-years-war-a-storm-of-swords]。或者你可能找不到，这种情况下，你可能会回去删除整个括号里的内容，换成“（可惜该播客已无法收听）”。

模型既不能上网搜索，也无法回头修改，所以它**只能猜测**。基础的大语言模型不会表现出半点迟疑，更不会加上免责声明说它只是在猜测，因为说到底，**模型每一步生成，本质上都是猜测**。只不过这次恰好发生在一个人类通常会切换到另一种文本生成模式的节点上做出的猜测，人类会去谷歌搜索，而不是凭感觉敲下键盘。

大语言模型非常擅长模仿它们在猜测对象中发现的任何模式。毕竟，这几乎就是它们被训练来做的事情。所以，如果它们编造一个社会安全号码，它会是一串看起来合乎规范的数字；如果它们编造一个播客的网址，它会看起来就像一个播客的网址。

在这个例子中，我试了 OpenAI 的 text-curie-001，一个 GPT-3 的小型变体，这个大语言模型将网址补全如下：

>  http://www.acast.com/the-rest-is-history-episode-5-the-Hundred-Years-War-1411-1453-with-dr-martin-kemp

这个地址中的 `dr-martin-kemp` Dr. Martin Kemp 在这里是一个真实存在的人吗？也许他是一位与历史播客有关的人？甚至就是我们正在谈论的那个播客的参与者？牛津大学确实有一位名叫 Martin Kemp 的艺术史学家，不过补全的内容是否就是指他，这听起来更像是一个语言理论问题，而不是大语言模型的问题（见[图 2-3](#figure2_3)）。无论如何，他并没有在播客 The Rest Is History 中谈论百年战争。


![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/522bea5da3214b4907f6bcbd42e342173860ccf8da3da5bb9ca932c100f1b203.jpg)  
<span id="figure2_3">图 2-3</span>. 人类的语言反映现实；模型的语言反映人类

### 幻觉

大语言模型被训练成“训练数据模仿机”，这一事实带来了一些不幸的后果：**幻觉 (hallucinations)**。

所谓幻觉，就是模型自信地生成一些看起来煞有介事、但实际上是错误的信息。无论是在临时使用大语言模型时，还是把它嵌入应用程序中，这都是一个常见的问题。

由于从模型的角度来看，幻觉与其他补全内容并无不同，所以像“不要瞎编”这样的提示指令效果非常有限。相反，典型的处理方法是**让模型提供一些可以核实的背景信息**。这可以是其推理过程的解释、可以独立进行的计算、来源链接，或是可以用于搜索的关键词和细节。例如，要核实“有一位英国国王娶了他的堂妹”这句话，比核实“有一位英国国王娶了他的堂妹，即乔治四世，他娶了不伦瑞克的 Caroline”要困难得多。对抗幻觉最好的解药是“信任，但要核实(Trust but verify)”，只不过要把这句中的“信任（rust）”去掉。

>译者注：
>
>这句话的原版是美国前总统里根常挂在嘴边的一句俄语谚语 — “Trust, but verify.”（信任，也要核实）。作者的意思是 “只保留核实，不要信任”，即面对大语言模型（LLM）的回答，要直接进入“核实”模式，不预设任何“它说的就是对的”信任。
>
>为什么要“去掉信任”？
> * LLM 随时可能产生幻觉：模型生成文本的机制是基于统计相关性，而不是事实检索；看似自信、条理清晰的回答也可能凭空捏造。
>
> * 信任本身会削弱警觉：如果先入为主地“信任”，就容易错过检查漏洞、数据来源和逻辑跳跃的机会。
>
> * 核实成本可控：与其事后发现错误带来重大损失，不如一开始就把“核实”当成标配流程。
>
>该如何核实？
>1. 要求证据：让模型列出推理步骤、参考来源或可复算的数据。
>2. 独立检索：对关键事实（年份、数值、人物关系等）进行搜索或查阅权威资料。
>3. 交叉对比：将模型答案与多个渠道的信息比对，留意是否一致。
>4. 标记不确定：若一时无法确认真伪，先标注为“待核实”，不要急于引用或发布。

幻觉也可能被诱导产生。如果你的提示词引用了不存在的东西，大语言模型通常会继续假设它存在。在写作中，开头提出错误主张，然后中途自我纠正的文档是很少见的。所以模型通常会**假设其提示词是真实的**，这被称为**真实性偏见 (truth bias)**。

你可以利用这种偏见为你服务——如果你想让模型评估一个假设或反事实的情景，没必要说“假装现在是 2030 年，尼安德特人已经被复活了”。直接开头写“现在是 2031 年，距离第一批尼安德特人被复活已整整一年”。

> #### **💡 提示**
>
> 如果你能接触到一个生成补全内容的大语言模型（即基础的大语言模型，而不是像 ChatGPT 那样包装在聊天界面里的），这或许是个好机会，可以尝试输入几个所谓的 **“假戏真做”提示词**。
>
> 就像前面关于复活尼安德特人的例子一样，“假戏真做”提示词不是直接提出假设性问题，而是通过暗示假设情景已经真实发生来引出答案。
>
> 将（基础的大语言模型）生成的补全内容与聊天大语言模型（如 ChatGPT）的回答进行比较。它有什么不同？

然而，大语言模型的真实性偏见也是危险的，尤其对于应用程序而言。在应用程序中，创建的提示词一旦写错，就可能无意间塞进去与事实不符或荒谬的元素。如果是一个人读到这样的提示词，他可能会放下手中的纸，对你挑起眉毛，然后说：“真的吗？” 但大语言模型没有这个选项。它会尽力假设这个提示词是真的，而且不太可能主动纠正你。所以，**你有责任给大语言模型一个不需要纠正的提示词**。

-----

## 大语言模型如何看待世界

在[“什么是大语言模型？”](#什么是大语言模型)一节中，你学到了大语言模型会处理和生成文本字符串。现在，我们有必要深入探究一下这句话的背后：**大语言模型是如何"看见"这些字符串的呢？**

我们习惯于将字符串看作是字符（characters）序列，但这并不完全是大语言模型眼中的样子。它能够对字符进行推理，但这并不是一种与生俱来的能力，需要大语言模型付出相当于高度集中的注意力，在撰写本文时（2024年秋），即使是最先进的模型，也仍然会被[“‘strawberry’中有多少个 R？”](https://www.inc.com/kit-eaton/how-many-rs-in-strawberry-this-ai-cant-tell-you.html)这类问题所迷惑。

事实上，我们人类其实也不是按字符来阅读字符串的。在人类大脑的早期处理阶段，字符就被组合成了单词。我们随后读的是单词，而不是字母。这就是为什么我们常常会忽略拼写错误而没有发现它们：在信息抵达意识层面之时，我们的大脑已经将它们纠正了。

如果你故意把句子打乱、但又保留单词边界（见[图 2-4](#figure2_4)，左），大多数人依旧能读懂，还能从中找乐趣。然而，如果你用一种忽视单词边界的方式来弄乱文本，读者就惨了，阅读体验会瞬间崩溃（见[图 2-4](#figure2_4)，右）。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/a4b7fa0e7c96e13ffa03d4f9a1a8034f625c8b2d611a00264db181804c2305db.jpg)  
<span id="#figure2_4">图 2-4</span>. 两种打乱同一文本的方式

```
Reschaersers at Notthangim Uvinesrtiy shoewd all the way bcak in the nieetneen svetneies taht yuo cuold scrambe Irettes wthiin a wrod and sitll be udnerostod.

Research ersat Nott inghamU niver sit ysho wedal Ithew aybac kinth e nin ete ens even ties tha tyo uco uldscr amblel et ter swith inaw ordandst ill eun derst ood.

```

图的左半部分保持了单词边界的完整，打乱了每个单词内部的字母顺序；而右半部分保持了字母顺序的完整，但改变了单词的边界。大多数人觉得左边的版本读起来要容易得多。

>译者注：
>
> 中文也可以达到左边示例的效果，原句：
>
>```
> 诺丁汉大学的研究人员早在20世纪70年代就证明，即使把单词内部的字母顺序打乱，人们仍然可以理解文本。
>```
> 打乱后仍然可读：
>
>```
> 诺汉丁大学的究研人员早在20世纪70年代就明证，即使把词单内部的母字顺序乱打，人们仍然以可理解文本。
>```

和人类一样，大语言模型也不逐个读取单个字母。当你向模型发送一段文本时，它首先被分解成一系列由多个字母组成的块，这些块被称为**Token（词元）**。它们通常有三到四个字符组成，但对于常见的单词或字母序列，也存在更长的 Token。模型使用的所有词元的集合，被称为它的**词汇表 (vocabulary)**。

当读取一段文本时，模型首先会通过一个**分词器 (tokenizer)**，将其转换成一个 Token 序列(a sequence of tokens)。完成这一步后，数据才会正式送进大语言模型本身。然后，大语言模型 LLM 会生成一系列 token（在内部表示为数字），这些 Token 在返回给你之前会被转换回人类可读的文本（见[图 2-5](#figure2_5)）。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/d866d6ceccf80190beee2cdf08933e885d168017b965db996e159732e9be54b6.jpg)  
<span id="#figure2_5">图 2-5</span>. 一个分词器将文本翻译成大语言模型处理的数字序列——然后再翻译回来

请注意，并非所有的分词器都会将开头的空格（whitespace）包含在 Token 内，但很多都会这么做。[OpenAI 的分词器](https://platform.openai.com/tokenizer)就是著名的例子。

>译者注：
>
>` apple` 分词后的词元不是 ` `（空格） 和 `apple`，而是一个以空格开头的词元` apple`
>
> 另外常用的是： [vercel 的分词器](https://tiktokenizer.vercel.app/)

大语言模型将文本看作是由 Token 组成的，而人类则将其看作是由单词组成的。这听起来好像大语言模型和人类看待文本的方式非常相似，但有几个关键的区别。

### 区别 1：大语言模型使用确定性的分词器

作为人类，我们将字母转换成单词的过程是模糊的。我们会试图找到一个与我们看到的字母序列最相似的单词。而另一方面，大语言模型使用**确定性的分词器**——这使得拼写错误格外醒目。单词 `ghost` 在 OpenAI 的 GPT 分词器（一个被广泛使用的分词器，不仅限于 OpenAI 的模型）中是一个单独的 Token。然而，拼写错误的 `gohst` 被翻译成 `g`-`oh`-`st` 这三个 Token 的序列——这显然是不同的，使得大语言模型很容易发现这个拼写错误。

尽管如此，大语言模型通常对拼写错误有相当强的适应能力，因为它们在训练集中已经习惯了这些错误。

### 区别 2：大语言模型无法放慢速度检查字母

我们人类可以放慢速度，有意识地逐一检查每个字母，但大语言模型只能使用其内置的分词器（而且它也无法放慢速度）。许多大语言模型已经从训练集中学会了某个 Token 由哪些字母组成，但凡是需要把 Token 再度拆分或重新拼装的语法任务变得异常困难。

[图 2-6](#figure2_6) 中有一个很好的例子，它描绘了一段关于反转单词字母的 ChatGPT 对话。反转字母是一种简单的模式操纵，大语言模型通常非常擅长这个。但事实证明，分解和重组词元对大语言模型来说太难了，所以无论是正向反转还是反向反转都错得离谱。

>译者注：
>
>中文可能做到句子中的文字完全颠倒。关键还是看大模型的分词器如何拆分中文token
>以下面这句话为例，在 GPT o3 模型下也做不到完全颠倒：
>```
>诺丁汉大学的研究人员早在20世纪70年代就证明，即使把单词内部的字母顺序打乱，人们仍然可以理解文本。
>
>是否可以从后到前，倒序输出上面的文字：。本文解理...
>```
>输出：
>
>`。本文解理以可然仍们人，乱打序顺母字的部内词单把使即，明证就代年07纪世02在早员人究研的学大汉丁诺`

在图中，最初的反转和再次反转都充满了错误。作为应用程序构建者，你在这里得到的启示是，**如果可以的话，避免让模型承担这类涉及 Token 内部层面的任务**。

> #### **💡 提示**
>
> 如果你希望大语言模型执行的任务当中，需要模型拆开再重新拼接 Token，最好考虑把这一步放在前置或后置处理中，由你自己来完成。


![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/d1b288878ca363ad7d82dfb9ce0e06ff79fe34430adc2391400949da02fce234.jpg)  
<span id="figure2_6">图 2-6</span>. ChatGPT 尝试反转字母但失败了

举个如何使用这个提示词的例子，假设你的应用程序正在使用大语言模型玩一个像 Scattergories 这样的游戏，游戏的目标是找出符合特定语法属性的示例，比如“以 W 开头的禁酒活动家”、“以 Sw 开头的欧洲国家”，或者“名字中包含 3 个字母 R 的水果”。

这时，更合理的方法是先让你的大模型充当“信息来源”，获取一个庞大的禁酒活动家或欧洲国家列表，然后用你自己的语法逻辑（代码）对这份清单进行过滤。如果你试图让大语言模型承担全部负担，它可能会频频出错（见[图 2-7](#figure2_7)）。

请注意，图中的模型是非确定性的，它以两种不同的方式失败了（见第一次和第二次尝试）。另请注意，[ Sweden]、[ Switzerland] 和 [ Somalia] 在 ChatGPT 的分词器中都是单个 Token。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/b8e2ac2edf16dd2b16823334220054aa1ec8b280b25cfd5d90a69903faef8443.jpg)
<span id="figure2_7">图 2-7</span>. ChatGPT 在识别以 “Sw” 开头的国家时遇到困难

### 区别 3：大语言模型看待文本的方式不同

我们想强调的最后一个区别是，我们人类对 Token 和字符的许多方面有直观的理解。我们用眼睛直接“看见”它们，所以能分辨哪些字母是圆润的、哪些是方正的。也正因此，我们能欣赏 ASCII 艺术（尽管许多模型可能已经死记硬背了大量的 ASCII 艺术）。对我们来说，加重音符号的字母只是同一个字母的一个变体，我们在阅读一篇充满重音的文本时，也不会太吃力。而另一方面，模型即使能够处理，也必须消耗可观的算力，从而减少了用于你所关心的实际应用的资源。

>译者注：
>
>重音符号指的是附加在字母上、下或旁边的那些小标记，用来改变字母的发音或区分单词。
>
>**举几个常见的例子：**
>
>* **é** (例如法语中的 *café* - 咖啡)
>* **è** (例如法语中的 *mère* - 母亲)
>* **ü** (例如德语中的 *über* - 关于，或者汉语拼音中的 *lǜ* - 绿)
>* **ñ** (例如西班牙语中的 *español* - 西班牙语)
>* **ç** (例如法语中的 *garçon* - 男孩)
>
>**所以，原文那句话的意思是：**
>
>对我们人类来说，我们看到 `e`、`é` 和 `è` 时，会直观地认为它们都是字母 “e” 的**变体**。我们的大脑可以毫不费力地将它们关联起来。
>
>但对于 LLM 来说，它的分词器（tokenizer）可能会将 `e` 视作一个词元（token），而将 `é` 视作一个完全**不同**的 token。这会导致模型需要花费更多的“精力”（处理能力）去理解“é 只是 e 的一种形式”，而这对人类来说是轻而易举的。

>译者注：
>
>ASCII art是一种**“用文本字符作画”**的艺术形式。
>简单的例子：`:)` 或 `:-)` 是笑脸的意思。
>
>这是一个**猫**的例子：
>
>```
>    /\_/\
>   ( o.o )
>    > ^ <
>```

这里一个特殊的例子是**大小写**。看看[图 2-8](#figure2_8)。为什么这个简单的任务会搞得……我是说……搞得这么糟？在继续阅读前，不妨结合之前的分词陷阱，自己先猜一猜原因。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/2d763025bc8a3836f65871d528a6a6ef27ad8cf4fda758bb294de2b932033d01.jpg)  
<span id="figure2_8">图 2-8</span>. 要求 OpenAI 的 text-babbage-001 模型将文本转换为全大写

这会产生一些有趣且典型的错误——请注意，我们为了演示目，使用了一个非常小的模型，较大的模型通常不会这么容易被难倒。

对人类来说，大写字母 `A` 只是小写字母 `a` 的一个变体，可对模型而言，包含大写字母的 token 与包含小写字母的 token 截然不同。虽然模型在训练数据中已经见过大量相关例子，它们早已意识到：句号后的 token `For` 与句子中间的 token `for` 非常相似。

然而，大多数分词器并没有让模型轻易地学会这些联系，因为大写 token 并不总是与非大写 token 一一对应。例如，GPT 分词器将 “strange new worlds” 翻译为 [str][ange][ new][ worlds]，这是四个 token。但全部大写后，分词变为 [STR][ANGE][ NEW][ wOR][L][DS]，这是六个 token。同样，单词 `gone` 是一个 token，而 `[G][ONE]` 是两个。

更强的大语言模型在处理这些大小写问题上表现得更好，但这一过程仍然会分走它们对真正问题的注意力，而你的问题很可能与大小写无关。（毕竟，你不需要一个大语言模型来帮你转换文本大小写！）所以，明智的提示词工程师会尽量避免让大语言模型一直忙于大小写转换，从而给模型增加过多负担。

### 计算 Token 数量

你无法随意“混搭”分词器和模型。**每个模型都使用一个固定的分词器**，所以了解你模型的分词器非常值得。

在编写大语言模型应用程序时，你可能需要在提示词工程过程中反复调用分词器，使用像 [Hugging Face](https://huggingface.co/docs/transformers/main_classes/tokenizer) 或 [tiktoken](https://github.com/openai/tiktoken) 这样的库。然而，分词器最常见的用途并不是复杂的 token 边界分析，而是更朴素也更重要的一件事：**计数**。

这是因为从模型的角度来看，**文本“多长”完全取决于 token 数量**。这包括了长度的各个方面：模型通读提示词所花的时间与提示词中的 token 数量大致成线性关系。同样，它生成回答所花的时间也与生成的 token 数量成线性关系。计算成本也是如此：一次预测所需的计算能力与输出 token 数量成正比。这就是为什么大多数“模型即服务”的提供商按生成或处理的 token 数量收费。在撰写本文时，一美元通常可以购买 50,000 到 1,000,000 个输出 token，具体取决于模型。

最后，token 的数量对于**上下文窗口（context window）**（即大语言模型在任何给定时间可以处理的文本量）这个问题至关重要。这是所有现代大语言模型的一个限制，我们将在本书中反复提及。

大语言模型并不仅仅是“给什么文本就生成什么文本”。它接收的文本的 token 数量必须小于上下文窗口的大小，并且它的补全内容要保证**提示词加上补全内容**的 token 总数也不能超过上下文窗口的大小。上下文窗口的大小通常以数千个 token 为单位来衡量，理论上讲，这个数量相当可观，不容小觑：它相当于几页，通常是几十页，有时是几百页 A4 纸大小的文本。但在实践中，它却往往不够用：无论你的上下文窗口有多大，你总会忍不住想把它填满，甚至填超，所以你需要计算 token 来防止这种情况发生。

>译者注：
>
>LLM 受限于“上下文窗口（context window）”——一次最多能处理多少 token。
>
>输入限制：提示中的 token 数必须 < 上下文窗口大小。
>
>输出限制：提示 + 模型生成的 token 总和也不能超过上下文窗口大小。
>
>上下文窗口通常用“几千 token”衡量，理论上可装下数页甚至数百页 A4 文本。但实践中，你往往会不自觉地填满乃至溢出它，所以 实时统计 token 是防止溢出的唯一办法。


字符数和 token 数量之间并没有通用换算公式。它取决于文本和所用的分词器。上面链接的非常常见的 GPT 分词器，在分词英文自然语言文本时，**平均每个 token 大约有四个字符**，这很典型。较新的分词器可能会稍微高效一些（即，平均每个 token 可以有更多字符）。

>译者注：
>
>中文因字符集超大，平均 1 字符需要 1–2 token，成本和上下文占用都显著高于英文，实际应按 1.5 token/字 左右预留空间

大多数分词器都为英语进行了优化，在其他语言上的效率通常较低，也就是说每个 token 含的字符更少。随机的数字字符串效率更低，每个 token 只有两个多字符。随机的数字+字母混合串（如加密密钥）更糟，通常每个 token 不到两个字符。含有罕见字符的字符串每个词元的字符数最少——例如，Unicode 笑脸符号🙂 就会被拆成两个 token。

> #### 📝 **注意**
>
> 大多数大语言模型使用的词汇表至少包含几个特殊词元：最常见的是至少一个**文本结束词元**，在训练中，这个词元被附加到每个训练文档的末尾，以便模型学习文档何时结束。每当模型输出该词元时，补全就会在该点被切断。
>
> 一个看似普通的占位符字符串 <|endoftext|>，实则由 tokenizer 分词器保留；在 `GPT-2/3` 里 ID 为 50256，在新版 `cl100k_base` 里为 100257；


## 一次一个 Token

让我们再剥下一层洋葱皮，这是我们触及核心之前的最后一层。

在底层，大语言模型并非直接进行“文本到文本”的转换，甚至也不是真正的“token 序列到 token 序列”。它是**多个 token 到一个新 token**。模型不断重复同一操作来生成下一个 token，并根据需要不断累积这些单个 token，直到组合成一段完整的文本输出。

### 自回归模型（Auto-Regressive Models）

大语言模型每运行一次，只会给出统计上最可能的下一个 token。然后，这个 token 会被追加到提示词（prompt）尾部，模型再运行一次，以获取在新提示词下统计上最可能的下一个词元，依此类推（见[图 2-9](#figure2_9)）。这种一次预测一个 token、且下一次预测依赖于前一次预测的过程，被称为**自回归 (autoregressive)**。

想想当你在手机上打字时，键盘上方会跳出三个候选词，**运行一个大语言模型，就好比不断地点击中间那个（最有可能的）候选词**。

这种“一步一个 token”的节奏与人类打字截然不同：人类可能会停下来检查、思考或反思，而**模型每一步都必须产出一个 token**。如果需要更长时间思考，大语言模型既拿不到额外的“思考时间”，也无法暂停。


![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/bd475b0fe6521836555e29ce79daa1e60fa218b73a01deb1f38dc82cb4918d40.jpg)  
<span id="figure2_9">图 2-9</span>. 大语言模型一次一个 token 地生成它们的回复

而且，一旦输出了一个 token，这个 token 就“板上钉钉”。**大语言模型无法回溯并删除这个 token**。它也不会发布纠正声明，声明它之前输出的内容是错误的，因为它没有在大量包含“作者明确撤回错误”场景的文档上训练——毕竟，写那些文档的人类可以在错误发生的地方回溯并纠正错误，所以在最终完成的文档中，这种明确的“撤回”（takebacks）是极为罕见的。

这个特性可能让大语言模型显得固执，甚至有点可笑，尤其当它们持续探索一条明显没有意义的路径时。但实际上，这意味着，在必要时，这种错误识别和回溯的能力需要由应用程序设计者——也就是**你**——来提供。

### 模式与重复（Patterns and Repetitions）

自回归系统的另一个问题是，它们可能会陷入自己的模式中。

大语言模型擅长识别模式，所以它们有时会（偶然地）创造出一个模式，然后找不到一个合适的时机跳出这个模式。毕竟，一旦模式形成，在任何给定的 token 处，继续这个模式的可能性都比打破它要大。结果就会出现极度重复的输出（见[图 2-10](#figure2_10)）。

> **以下是我喜欢《星际迷航：下一代》的理由列表：**
>
> 1.  角色丰满且有趣。
> 2.  情节结构精良且引人入胜。
> 3.  背景设定完善且迷人。
> 4.  剧集编写精良且娱乐性强。
> 5.  音乐谱写优美且恰如其分。
> 6.  视觉效果一流。
> 7.  整体制作价值很高。
> 8.  该系列拥有强大而忠实的粉丝群。
> 9.  该系列拥有丰富的历史。
> 10. 该系列拥有光明的未来。
> 11. 该系列拥有坚实的基础。
> 12. 该系列拥有热情的粉丝基础。
> 13. 该系列拥有强大的遗产。
> 14. 该系列拥有悠久的历史。
> 15. 该系列拥有丰富的神话体系。
> 16. 该系列拥有强大的演员阵容。
> 17. 该系列拥有强大的遗产。
> 18. 该系列拥有强大的追随者。
> 19. 该系列拥有光明的未来。
> 20. 该系列拥有坚实的基础。
> 21. 该系列拥有热情的粉丝基础。
> 22. 该系列拥有强大的遗产。
> 23. 该系列拥有强大的追随者。
> 24. 该系列拥有光明的未来。
> 25. 该系列拥有坚实的基础。
> 26. 该系列拥有热情的粉丝基础。
> 27. 该系列拥有强大的遗产。
> 28. 该系列拥有强大的追随者。
> 29. 该系列拥有光明的未来。
> 30. 该系列拥有坚实的基础。
> 31. 该系列拥有热情的粉丝基础。
> 32. 该系列拥有强大的遗产。
> 33. 该系列拥有强大的追随者。
> 34. 该系列拥有光明的未来。
> 35. 该系列拥有坚实的基础。
> 36. 该系列拥有热情的粉丝基础。
> 37. 该系列拥有强大的遗产。
> 38. 该系列拥有强大的追随者。
> 39. 该系列拥有光明的未来。
> 40. 该系列拥有坚实的基础。
> 41. 该系列拥有热情的粉丝基础。
> 42. 该系列拥有强大的遗产。
> 43. 该系列拥有强大的追随者。
> 44. 该系列拥有光明的未来。
> 45. 该系列拥有坚实的基础。
> 46. 该系列拥有热情的粉丝基础。
> 47. 该系列拥有强大的遗产。
> 48. 该系列拥有强大的追随者。

>Here's a list of reasons why I like Star Trek TNG:
>
>1. The characters are well-developed and interesting. 
>2. The plot is well-constructed and engaging. 
>3. The setting is well-realized and fascinating. 
>4. The episodes are well-written and entertaining. 
>5. The music is well-composed and fitting. 
>6. The visual effects are top-notch. 
>7. The overall production values are high. 
>8. The franchise has a strong and loyal following. 
>9. The franchise has a rich history. 
>10. The franchise has a bright future. 
>11. The franchise has a strong foundation. 
>12. The franchise has a passionate fanbase. 
>13. The franchise has a strong legacy. 
>14. The franchise has a long history. 
>15. The franchise has a rich mythology. 
>16. The franchise has a strong cast. 
>17. The franchise has a strong legacy. 
>18. The franchise has a strong following. 
>19. The franchise has a strong future. 
>20. The franchise has a strong foundation. 
>21. The franchise has a passionate fanbase. 
>22. The franchise has a strong legacy. 
>23. The franchise has a strong following. 
>24. The franchise has a strong future.
>25. The franchise has a strong foundation. 
>26. The franchise has a passionate fanbase. 
>27. The franchise has a strong legacy. 
>28. The franchise has a strong following. 
>29. The franchise has a strong future. 
>30. The franchise has a strong foundation. 
>31. The franchise has a passionate fanbase. 
>32. The franchise has a strong legacy. 
>33. The franchise has a strong following. 
>34. The franchise has a strong future. 
>35. The franchise has a strong foundation. 
>36. The franchise has a passionate fanbase. 
>37. The franchise has a strong legacy. 
>38. The franchise has a strong following. 
>39. The franchise has a strong future. 
>40. The franchise has a strong foundation. 
>41. The franchise has a passionate fanbase. 
>42. The franchise has a strong legacy. 
>43. The franchise has a strong following. 
>44. The franchise has a strong future. 
>45. The franchise has a strong foundation. 
>46. The franchise has a passionate fanbase. 
>47. The franchise has a strong legacy. 
>48. The franchise has a strong following.

<span id="figure2_10">图 2-10</span>. 由 OpenAI 的 text-curie-001 模型生成的一份理由列表（为了演示目的，选择了一个较旧的模型，因为新模型很少会如此尴尬地陷入重复陷阱）

在图中，大语言模型生成了一份喜欢某个电视剧的理由清单。你能发现几种模式？以下是我们找到的：* 每一条都是连续编号的语句，而且都能在一行内写完，这一点很理想。
  * 全部以 “The” 开头，这倒还能接受。
  * 句式都是 “X is Y and Z.”，这就让人头疼，因为可能会影响准确性。如果没有合适的 Z，模型可能凭空捏造。不过它在第 5 条后就停了。
  * 连续出现几条以 “The franchise” 开头后，后面的条目就全都这样开头——这也太蠢了。
  * 到了列表尾部，“legacy”“following”“future”“foundation”“fanbase” 这些词被疯狂重复，同样很蠢。
  * 列表没完没了，因为写完一条后，模型更倾向于继续而不是结束，而且它永远不会觉得无聊。

解决这种重复性输出的常见方法是检测并过滤掉它们；另一种思路是在结果中加入一些随机性。我们将在下一节讨论如何随机化输出。

-----

## 温度与概率（Temperature and Probabilities）

在上一节中，你学到大语言模型会计算最可能的词元。但如果你再剥开一层大语言模型这颗洋葱，你会发现，实际上，它在选择单个词元之前，模型其实会先计算**词汇表中所有 token** 的概率分布，然后才选定其中一个。这个在幕后选择实际词元的过程被称为**采样 (sampling)**（见[图 2-11]($figure2_11)）。


![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/b5baa1b86f96ea7629b83479cc77c3c0a6269fd993ffd4deb54912fabfef9157.jpg)  
<span id="figure2_11">图 2-11</span>. 采样过程实况

请注意，大语言模型不只是计算最可能的词元；它计算所有词元的可能性。

许多模型会把这些概率公开给你。模型通常以 **logprobs**（即词元概率的自然对数）的形式返回。logprob 越高，表示模型认为这个词元出现的可能性就越大。Logprobs 永远不会大于 0，因为 logprob 为 0 意味着模型百分之百确信这就是下一个词元。一般来说，最可能的词元的 logprob 会在 $-2$ 和 0 之间（见[图 2-12](#figure2_12)）。

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/424d42ac79f3129b11ba41c3d684e5dde318fd1c4f8e4467891c7518e152f5cd.jpg)  
<span id="figure2_12">图 2-12</span>. 一个请求 logprobs 并提取所选补全内容 logprobs 的 API 调用示例

请注意，在图中，将请求参数 logprobs 设置为 3 意味着将返回前三个最可能词元的 logprobs。

然而，你可能并不总是想要最可能的那个词元。特别是如果你有办法自动测试你的补全内容，你可能会想生成几个候选项，然后再把差劲的删掉。实现这一点的典型方法是使用一个大于 0 的**温度 (temperature)**。

温度是一个不小于零的数字，它决定了模型应该有多“创意”。更具体地说，如果温度大于 0，模型会给出一个**随机性的补全**，它会倾向于选择最高的概率的词元，但也可能会返回一些可能性稍低但并非完全荒谬的词元。温度越高，且候选词元之间的 logprobs 越接近，排名第二的词元被选中的可能性就越大，甚至第三、第四或第五名也可能被选中。精确的公式如下：

$$p(\mathbf{t o k e n}_i) = \frac{\exp(\mathrm{l o g p r o b}_i / t)}{\sum_j\exp(\mathrm{l o g p r o b}_j / t)}$$

让我们看看可能的温度值以及你应该在什么时候选择它们：

  * **0**
    你想要**最可能**的词元。没有其他选择。当**正确性至关重要**时，这是推荐的设置。此外，在温度为 0 的情况下运行大语言模型接近于确定性输出，在某些应用中，可重复性是一个优势。

  * **0.1-0.4**
    如果有一个备选词元仅比领跑者可能性略低，你希望有**微小的机会**选中它。一个典型的用例是，你想要生成少量不同的解决方案（例如，因为你知道如何筛选出最好的一个）。或者，也许你只需要一个补全，但希望它比在温度 0 时预期的更丰富多彩、更有创意。

  * **0.5-0.7**
    你希望**偶然性**对解决方案有更大的影响，并且你能够接受得到的补全内容“不那么精确”，换句话说，模型即便认为另一个 token 更合理，也可能选了概率更低的那个。典型场景是一次性需要大量（通常 10 个以上）相互独立的答案。

  * **1**
    你希望词元分布能**严格反映训练语料中的统计分布**。举个例子，假设你的前缀是“One, Two,”，在训练集中，有 $51%$ 的情况下后面跟着词元 [ Buck]，有 $31%$ 的情况下跟着 [ Three]（假设模型已充分学会这一统计规律）。如果你在温度为 1 的情况下多次运行模型，那么 $51%$ 的时间你会得到 [ Buck]， $31%$ 的时间你会得到 [ Three]。

  * **\>1**
    你想要一个比训练集\*\*“更随机”\*\*的文本，就把温度设定到 1 以上。这意味着模型不太可能选择“标准”的续写，而更有可能选择一个比训练集中的典型文档“特别奇怪”的续写。

高温度会让大语言模型听起来像是喝醉了。在温度大于 1 的情况下进行长文本生成时，错误率通常会随着时间推移而恶化。原因在于，温度只作用于最后一层——将概率分布转化为具体输出 token 的那一步，它不影响大语言模型计算这些概率的主要处理过程。所以模型会把自己刚刚生成的错误当成一种模式来模仿，并继续在高温度的影响下叠加更多错误（见[图 2-13](#figure2_13)）。

>注释：
> logprob 与 temperature
>
> 什么是 logprob？
>
>   token 真正出现的概率 P, 取值 0<p≤1 。
>
> 为什么不用直接返回 $p$？
>
>   多个 token 概率很小（例如 $10^{-8}$ 级别）时，浮点数易下溢；换成对数即变成 $-18$ 左右，数值稳定。
>
> 如何把 logprob 变回概率？
>    $$
>     p = e^{\text{logprob}}
>   $$
>
> logprob 与 temperature 的关系
>
> **先有 logprob，后调温度**
>
>   * LLM 主体根据上下文计算每个 token 的 *logprob*（或者说 *logit*）。
>   * temperature 仅在“softmax 转概率”这一步生效；它并不改变 *logprob* 本身，也不影响模型对上下文的理解。
>
>小结
>
>* **logprob** 描述“模型眼里每个 token 的原始可信度”，取值 ≤ 0。
>* **temperature** 通过缩放 logprob，在采样阶段控制输出的确定性 vs. 随机性。
>* 二者搭配，既能精确复刻训练语料的统计模式，也能按需制造更多样、更富创造力或完全确定的输出。理解它们的配合机制，是设计可靠 AI 智能体提示词、生成策略与自动质检流程的基础。

<br>

![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/5721038ff672c109396836d7d474b0591095f34010bc5e9a18172599bd0331ba.jpg)  
<span id="figure2_13">图 2-13</span>. 高温对大语言模型的影响有点像酒精对人类的影响

Prompt：
饮酒会以多种方式改变人们的行为。

例如：

温度设置为 0.0 时采样：

1. 饮酒过量会导致判断力受损，致使人们做出通常情况下不会做出的决定。

温度设置为 1.0 时采样：

2. 饮酒可能导致攻击性和暴力行为增加，尤其是在某人饮酒过多的情况下。

温度设置为 2.0 时采样：

3. 口齿不清也可能是另一种明确的行为表现，【此后为无逻辑的乱码，无法翻译】。



该图显示了在高温下的这种退化现象，在第 3 项处，生成的文本一开始虽然错误不少但仍可辨认，最后却连单词都看不出来。。请注意，图中的每一项都是用 OpenAI 的 text-davinci-003 模型中以越来越高的温度采样得到的。

让我们回到模型写清单的例子。文本中一个典型的清单会在几项后停止，比如 3、4 或 5 项。如果是一个更长的清单，那么 10 是下一个最明显的停止点。当模型写完一行后，它可以选择两条路：继续输出下一个数字作为下一个 token 来扩展列表，或是输出第二个换行符（也可能是其他符号）来宣告列表结束。

在温度为 0 时，大语言模型会始终挑选它认为在当前行「最可能」的选项。通常，结果往往是它会一直续写，尤其在超过最后一个明显停点之后。

在温度为 1 时，如果大语言模型判断继续的概率是 $x$，那么它将仅以概率 $x$ 继续。所以，随着条目不断增多，模型很可能会在某个时刻停下来，其最终列表长度大体与训练语料中的典型长度相当。总的来说，里存在一种权衡（见[表 2-1](#table2_1)）。

**<span id="table2_1">表 2-1</span>. 不同温度设置的优缺点**

| 高温 | 低温 |
| :--- | :--- |
| + 更多备选项。 | + 更正确的解决方案。 |
| + 生成的许多属性（如列表长度）与训练集中的分布相同。 | + 更具可复现性（确定性）。 |

还有其他的采样方式，最著名的是**束搜索 (beam search)**，它试图解决这样一个问题：选择一个看起来很有可能的特定词元，可能会使下一步的选择变得困难，因为后面缺乏合适的衔接 token。束搜索做法是向前预测接下来的多个 token，确保存在一条整体概率较高的候选序列。这可以带来更准确的解决方案，但由于其更高的时间和计算成本，在应用中较少使用。

-----

## Transformer 架构

是时候切开洋葱的最后一层，直接看看大语言模型的大脑了。你剥开它，发现……它根本不是一个整体的大脑。它是**成千上万个小脑**。所有这些小脑在结构上完全相同，每个都在执行非常相似的任务。序列中的每个词元上，都坐着这样一个小脑。这些小脑组合在一起，共同构成了 **Transformer**，这是所有现代大语言模型使用的架构。

每个小脑开始时，都会被告知它坐在哪个词元上，以及它在文档中的位置。这个小脑会思考固定的步数，这些步数被称为**层 (layers)**。在这个过程中，它可以从左边的小脑接收信息。小脑的任务是：从其所在位置的角度来理解文档，它通过两种方式来理解：

  * 在除最后一步之外的所有步骤中，它会与右侧的小脑分享其部分中间结果。（我们稍后会更详细地讨论这一点。）
  * 在最后一步，它被要求预测其紧邻右侧的词元会是什么。

每个小脑都经历着相同的过程：计算和分享中间结果，然后进行猜测。事实上，这些小脑是彼此的克隆体，它们的处理逻辑是相同的，唯一的不同是输入：它们各自起始的词元，以及它们从左边的小脑那里得到了哪些中间结果。

但它们经历这些步骤的原因是不同的。位于最右边、最后一个词元上的小脑，运行的目的是预测下一个词元。它分享的中间结果不重要，因为右边没有小脑在听。但对于所有其他小脑来说，情况恰好相反，它们的目的是把自己的中间结果分享给右侧的小脑，而它们对紧邻其后词元的预测并不重要，因为那个的词元是已知的。

当最右边的词元做出预测时，[“一次一个词元”](#一次一个词元)中的自回归机制就启动了：它吐出新的词元，一个全新的小脑被放置在该词元之上，以在固定的层数内深化对当前位置内容的理解。之后，它再去预测下一个词元。重复同样的流程——或者更确切地说，**缓存并重复**，因为这一次的计算结果会在接下来的提示词以及生成的补全内容的每个词元中一次又一次地被重复利用。

[图 2-14](#figure2_14) 展示了该算法的一个例子，其中每一列代表一个小脑，展示其状态在时间轴上的变化。在这个例子中，你刚刚要求模型补全“One, Two,”，最终你会得到两个词元 [ Buck]（带前导空格） 和 [le ]（带尾随空格）。让我们跟随 Transformer 的推理过程，看看它是如何得出这一结果的。

在四个输入词元上各坐着一个小脑：[One]、[, ]、[Two] 和 [, ]（最后一个`[, ]`是同一个词元的第二次出现）。每个小脑都思考四层，连续地加深它们对正在处理的文本的理解。在每一步中，它们都会从左边的词元那里更新到目前为止学到的东西。它们每个都会计算一个猜测，猜测右边的词元可能是什么。

前几个猜测是针对仍然是提示词一部分的词元：[One]、[, ]、[Two] 和 [, ]。我们已经知道提示词了，所以这些猜测就被扔掉了。但接着，模型来到了补全阶段，在这里，猜测才是重点。所以这次猜测被转化为一个预测，即词元 [ Buck]。一个新的小脑被安放在这个词元之上，经过它四个步骤的思考，得出了下一次预测 [le ]。如果你继续补全，另一个小脑将被安放在 [le ] 之上，以此类推。


![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/ebb941eb7d4796a57acd0f002cfb4e6306ca12a9cddcc5c2d21521879a4c0d97.jpg)  
<span id="figure2_14">图 2-14</span>. 模型生成一个词元的内部工作原理——较后的层绘制在较前的层之上

现在，让我们回过头来谈谈在小脑之间共享的“中间结果”。它们共享的方式被称为**注意力机制 (attention mechanism)**——这是 Transformer 架构对大语言模型的**核心创新**（如[第一章](./Chapter%201.%20Introduction%20to%20Prompt%20Engineering.md)所述）。注意力机制是在小脑之间传递信息的一种方式。当然，可能有成千上万个小脑，每个小脑都可能知道一些对其他所有小脑都有用的东西。为了防止这种信息交换陷入混乱，整个过程必须受到严格的调度与控制。它是这样工作的：

1.  每个小脑都有一些它想知道的内容，所以它会提交几个**问题**，希望其他小脑能回答。比方说，一个小脑坐在词元 [my] 上。这个小脑想知道 [my] 可能指的是谁，所以一个合理的问题是：“谁在说话？”

2.  每个小脑都有一些它可以分享的东西，所以它会提交几条**信息**，希望对其他小脑有用。比方说，一个小脑坐在词元 [Susan] 上，它之前已经知道这个词元是一句介绍的最后一个词，比如“Hello, I'm Susan.”。因此，为了可能帮助后面的小脑，它会提交信息：“现在说话的人是 Susan。”

3.  现在，每个问题都会与最匹配的答案配对。“谁在说话？”与“现在说话的人是 Susan”非常匹配。

4.  每个问题的最佳答案会反馈给提问的小脑，所以位于词元 [my] 的小脑被告知“现在说话的人是 Susan。”

当然，虽然这个例子中的小脑用英语交流，但实际上，它们使用一种由长串数字向量组成的“语言”，这种语言对每个大语言模型都是独一无二的，因为这是大语言模型在训练期间“发明”出来的。

> #### 📝 **注意**
>
> 信息永远只从左向右流动。
> 信息永远只从下向上流动。

在现代大语言模型中，这种问答机制还遵循另一个约束，称为**掩码 (masking)**：并非所有的小脑都能回答一个问题；只有位于提问小脑**左侧**的那些才能回答。而且，一个小脑永远不会被告知它的答案是否被使用，所以右边的小脑永远不能影响左边的小脑。

这种单项流动有一些实际的后果。例如，要计算一个小脑在某一层的状态，模型只需要其左侧（该层更早的小脑）和下方（前一层中同一个小脑）的状态。这意味着部分计算可以**并行**进行——这也是生成式 Transformer 训练效率如此之高的原因之一。在任何时间点，已经计算完成的阶段都会形成一个三角形（见[图 2-15](#figure2_15)）。


![](https://cdn-mineru.openxlab.org.cn/extract/53a88176-f017-418a-92e5-740d1baf8375/1354ecc455328c6427d4a25fc4f28eba2f9f05ea1c54d32eefac35fe7c868359.jpg)  
<span id="figure2_15">图 2-15</span>. 计算大语言模型的内部状态

在图中，首先（左上角），只能计算第一个词元的最底层。接着（上方中间），第一个词元的第二底层和第二个词元的最底层都可以计算。再过一步（右上角），第一个词元的第三层、第二个词元的第二层和第三个词元的第一层都可以计算……直到所有状态都计算完毕，可以抽样生成一个新词元。

并行计算带来了速度提升，但是当模型从读取提示词切换到创建补全内容时，这种三角形的计算方式就失效了。模型必须等到一个词元的所有层都被处理完毕，才能选择下一个词元，并计算新小脑的第一个状态。这就是为什么**大语言模型通读长提示词的速度远快于生成长补全内容的速度**。两者的速度与词元数量呈线性增长，但读取提示词的词元速度要快一个数量级左右。

这种三角形结构反映了大语言模型一种普遍的“向后和向下”的视野方向，或者一个更好的理解方式是“向后和向笨”：

  * **向后 (Backward)**

    小脑只能向左看。它们可以回看任意远，但绝不能向前看。这就是人们称 GPT 或其他大语言模型为**单向 Transformer** 的原因。信息永远不会从右边的小脑传递到左边的小脑。这使得生成式 Transformer 易于训练和运行，却深刻影响了它们处理信息的方式。

  * **向下（“向笨”）(Downward ("dumbward"))**

    在同一层里，一个小脑要得出自己的答案，只能参考排在它前面的那些小脑的答案。这意味着，如果我们将小脑在每一层所做的思考算作一个推理步骤，那么在第 $i$ 层的任何“推理链”最多只能有 $i$ 个推理步骤。但是，小脑没有办法将一个在较高层级获得的见解提供给一个在较低层级的小脑进行进一步处理。当然，凡事无绝对，有一种方式可以：当大语言模型生成文本时，最高层的结果——词元——被输出，而这个输出的词，又构成了下一个小脑第一层最基础的输入信息。这种\*\*“大声说出思考”**是模型让信息从高层流向低层的唯一方式——可以说，它是在脑子里把想法反复琢磨。这让人想起一句俗语：“我得先听到自己说了什么，才知道自己在想什么。”，这个原理也正是**思维链提示 (chain-of-thought prompting)\*\* 的基础（见[第八章](#)）。

注释：
>理解“无法向下反馈”的限制
>
> >但是，“迷你大脑”完全没有办法将一个在较高层级中获得的见解，再反馈给一个较低层级的“迷你大脑”进行深加工。
>
>这正是上述“电梯只能上不能下”规则带来的直接后果。
>
>**举个例子：** 假设我们问模型一个数学题：“我有5个苹果，吃了2个，又买了4个，现在有几个？”
>
>模型在一次处理（一个向上流动的过程）中：
>.  **底层（比如第1-5层）**：识别出数字“5”、“2”、“4”，以及动词“吃”、“买”。
>.  **中层（比如第6-20层）**：理解了“吃”意味着减少，“买”意味着增加。它可能建立了 `(5, -2)` 和 `(?, +4)` 这样的关系对。
>.  **高层（比如第21-32层）**：尝试整合所有信息，进行最终计算。它在这里才“恍然大悟”：哦，这是一个连续的加减法，应该先算 `5-2`，得到结果 `3`，再用 `3` 去加 `4`。
>
>**这里的“限制”是**：当第30层的“迷你大脑”领悟到“应该先算 `5-2=3`”这个**高层见解**时，它**没有办法**把“3”这个中间结果送回给第3层，让第3层的员工用这个“3”去和“4”做下一步处理。它必须在自己这一层，根据底下所有层传上来的、未经整合的信息，一次性地完成 `5-2+4=7` 的整个推理和计算。
>
>对于简单问题，这或许可行。但对于复杂问题，这就像要求顶层CEO在不和下属沟通的情况下，仅凭一堆原始数据报表就瞬间做出完美的最终决策一样，非常困难且容易出错。
>
> ---
>
>理解“出声思考”这个唯一的变通方法
>
> >当然，凡事无绝对，有一种方式可以：当大语言模型生成文本时，最顶层的处理结果——也就是那个词（token）——被输出了出来，而这个输出的词，又构成了下一个“迷你大脑”第一层最基础的输入信息。这种“出声思考”是模型能让信息从高层流向底层的唯一方式...
>
>这就是模型实现复杂推理的**关键所在**。它虽然不能在一次处理中让信息向下流，但它可以通过**生成一个词，然后把这个词作为新的输入，开始一次全新的处理**。
>
>**我们继续用上面的苹果问题来解释这个“变通方法”：**
>
>这次，我们引导模型使用“思维链”（Chain of Thought），比如在问题后加上“请一步一步思考”。
>
>1.  **第一次处理（生成第一个中间步骤）**
>    * **输入**：“我有5个苹果，吃了2个，又买了4个，现在有几个？请一步一步思考。”
>    * **向上流动**：信息在32层楼里走了一遍。
>    * **顶层输出（成品）**：模型根据高层见解，决定最合理的“第一步”是先处理“吃掉2个”。于是，它生成了文字：“`好的，我们先算吃掉2个苹果后还>剩多少。一开始有5个，5 - 2 = 3个。`”
>
>2.  **第二次处理（利用上一步的结果）**
>    * **新的输入**：现在，模型的输入变成了：“我有5个苹果...请一步一步思考。**好的，我们先算吃掉2个苹果后还剩多少。一开始有5个，5 - 2 = >3个。**”
>    * **向上流动**：这个包含了中间结果“3个”的全新信息，被**重新送回第1层**，开始新一轮的32层加工。
>    * **这一次，所有层级的“迷你大脑”从一开始就能看到“剩下3个”这个明确的中间结论**。高层“迷你大脑”不再需要从头计算 `5-2`，它可以直接利用>这个已知的“3”，去处理问题中剩下的部分“又买了4个”。
>    * **顶层输出（成品）**：模型生成了下一步：“`现在我们有3个苹果，又买了4个，所以 3 + 4 = 7个。`”
>
>3.  **第三次处理（得出最终结论）**
>    * **新的输入**：包含了前面所有思考步骤的文本。
>    * **向上流动**：模型看到所有计算都已完成。
>    * **顶层输出（成品）**：模型生成最终答案：“`所以，现在总共有7个苹果。`”
>
>**这就是“出声思考”的本质：**
>模型将高层处理得到的“见解”（比如中间步骤 `5-2=3`），先以文字形式“说”出来（生成token）。然后，它再“听”到自己刚刚说的话（将生成的文字作为>下一次处理的新输入），这个宝贵的中间结果就能被送到最底层的“迷你大脑”，从而指导下一轮的、更进一步的思考。
>
>这个过程完美地诠释了那句“**我还没听见自己说什么，怎么知道我在想什么呢**”。模型必须先把一个想法（中间步骤）表达出来，才能基于这个已表达的>想法，继续思考下一步。这正是“思维链”提示法能极大提升大语言模型推理能力的核心原理。


让我们看一个例子。上面那段文字有多少个单词？如果你和我一样，大概率不会真的去数，你会期望作者直接告诉你。好吧，我们告诉你：是 173 个。但为了论证，你本可以自己抬头数一下的，对吧？

我们向 ChatGPT 提出了这个问题，方法是给它输入本章直到并包括“上面那段文字包含多少个单词？”。它回答说：“上面那段文字包含 348 个单词。” 不仅错了，而且错得离谱，无可救药。对于那段文字来说单词太多，但对于整篇文章来说又太少。

当然，这个要求对对大语言模型来说太苛刻。人类会做得更好。他们可以再读一遍文本，并在脑海里保持一个计数器。这对大语言模型行不通，因为它只读一遍文本，不能回头看。所以当小脑在唯一一次机会里处理那段文字时，它们不知道“单词数量”才是关键特征，因为这个请求出现在章节文本的**下方**出现。它们正忙于分析语义、语气、风格以及无数的表层特征，而没有将全部注意力放在真正重要的“计数”这件事情上。

这就是为什么**提示词的顺序**对提示词工程至关重要——它往往就决定了一个提示词是成功还是失败。事实上，当我在开头提出单词计数问题时……好吧，ChatGPT 仍然没有得到正确的答案，因为计数对大语言模型来说很难。但至少它接近了很多，声称是 173。在[第六章](#)中，我们会再次回到“如何安排提示词各部分顺序”这个主题。

> #### **💡 提示**
>
> 如果你想知道一项能力对大语言模型来说是否现实，问自己这个问题：
>
> **假设一个熟记所有相关通用知识的人类专家，能否在不回溯、不编辑、不记笔记的情况下，一次性完成这个提示？**

### 结论

我们在本章讨论了四个核心事实。第一，大语言模型是**文档补全引擎**。第二，它们**模仿**它们在训练期间见过的文档。第三，大语言模型**一次生成一个词元**，没有暂停或编辑先前词元的选项。最后，大语言模型只**从头到尾读一遍文本**，不回溯、不重读。让我们在下一章看看这些事实如何转化为一个通用的提示词工程范式。

-----
